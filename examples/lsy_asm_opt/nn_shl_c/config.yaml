# Configuration for function minimization example
max_iterations: 100
checkpoint_interval: 5
file_suffix: ".s"

# # LLM configuration
# llm:
#   primary_model: "gemini-2.5-flash-lite"
#   # primary_model: "gpt-5-mini"
#   # primary_model: "llama3.1-8b"
#   primary_model_weight: 0.8
#   secondary_model: "gemini-2.5-flash"
#   # secondary_model: "llama-4-scout-17b-16e-instruct"
#   # secondary_model: "gpt-5-nano"
#   secondary_model_weight: 0.2
#   api_base: "https://generativelanguage.googleapis.com/v1beta/openai/"
#   # api_base: "https://api.cerebras.ai/v1"
#   # api_base: "https://api.openai.com/v1"
#   temperature: 0.7
#   max_tokens: 16000
#   timeout: 120
# LLM configuration
llm:
  primary_model: "deepseek-chat"
  primary_model_weight: 1.0  # 设置为1.0，确保只用主模型
  # secondary_model: ""  # 注释或留空以禁用辅助模型
  secondary_model_weight: 0.0
  api_base: "https://api.deepseek.com"  # DeepSeek的API base URL（兼容OpenAI格式，可用"https://api.deepseek.com/v1"）
  temperature: 0.7
  max_tokens: 4096
  timeout: 120

# Prompt configuration
prompt:
  system_message: "You are an expert in aarch64 assembly optimization. Rewrite the entire program to optimize for performance."

# Database configuration
database:
  population_size: 50
  archive_size: 20
  num_islands: 3
  elite_selection_ratio: 0.2
  exploitation_ratio: 0.7

  # embedding_model: "text-embedding-3-small"
  similarity_threshold: 0.99

# Evaluator configuration
evaluator:
  timeout: 120
  # cascade_thresholds: [1.3]
  cascade_evaluation: false
  parallel_evaluations: 3

# Evolution settings
diff_based_evolution: false
max_code_length: 20000
